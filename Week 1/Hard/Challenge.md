# Was basque is esta
**Author:** Chris Simpkins ([Source](https://cs1331.gitlab.io/fall2018/hw2/hw2-source-model.html))

**Modified by:** Rurudu

## Background (By Chris Simpkins)
In machine learning we train a model on some data and then use that model to make predictions about unseen instance of the same kind of data. For example, we can train a machine learning model on a data set consisting of several labeled images, some of which depict a dog and some of which don’t. We can then use the trained model to predict whether some unseen image (an image not in the training set) has a dog. The better the model, the better the accuracy (percentage of correct predictions) on unseen data.

We can create a model of language and use that model to predict the likelihood that some unseen text was generated by that model, in other words, how likely the unseen text is an example of the language modeled by the model. The model could be of a particular author, or a language such as French or English. One simple kind of model is well-suited to this problem: Markov chains.

Markov chains are useful for time-series or other sequential data. A Markov chain is a finite-state model in which the current state is dependent only on a bounded history of previous states. In a first-order Markov chain the current state is dependent on only one previous state.

A Markov chain can be represented as a transition matrix in which the probability of state j after state i is found in element (i, j) of the matrix. In the example below we have labeled the rows and columns with letters for readability. The probability of seeing the letter n after the letter a in the training corpus is found by entering row a and scanning across to column n, where we find the probability .12.

~~~
 	a	b	c	d	e	f	g	h	i	j	k	l	m	n	o	p	q	r	s	t	u	v	w	x	y	z
a	0.19	0.19	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.12	0.01	0.12	0.01	0.01	0.01	0.06	0.01	0.19	0.06	0.01	0.06	0.01	0.01	0.01
b	0.12	0.12	0.01	0.01	0.24	0.01	0.01	0.01	0.12	0.01	0.01	0.18	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.12	0.01	0.06	0.01	0.06	0.01
c	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
d	1.00	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
e	0.11	0.22	0.01	0.01	0.11	0.01	0.22	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.11	0.22	0.01	0.01	0.01	0.01	0.01	0.01	0.01
f	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
g	0.40	0.20	0.01	0.01	0.01	0.01	0.01	0.01	0.40	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
h	0.75	0.25	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
i	0.01	0.01	0.01	0.01	0.10	0.01	0.30	0.01	0.01	0.01	0.01	0.01	0.01	0.20	0.01	0.01	0.01	0.01	0.01	0.40	0.01	0.01	0.01	0.01	0.01	0.01
j	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
k	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
l	0.01	0.01	0.01	0.01	0.50	0.01	0.01	0.01	0.38	0.01	0.01	0.12	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
m	0.01	1.00	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
n	0.01	0.01	0.01	0.17	0.01	0.01	0.01	0.01	0.17	0.01	0.01	0.01	0.01	0.17	0.01	0.01	0.01	0.01	0.33	0.17	0.01	0.01	0.01	0.01	0.01	0.01
o	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	1.00	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
p	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
q	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
r	0.33	0.67	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
s	0.50	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.50	0.01	0.01	0.01
t	0.10	0.20	0.01	0.01	0.01	0.01	0.01	0.20	0.01	0.01	0.01	0.20	0.01	0.01	0.10	0.01	0.01	0.01	0.01	0.20	0.01	0.01	0.01	0.01	0.01	0.01
u	0.01	0.33	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.33	0.33	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
v	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
w	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.50	0.50	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
x	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
y	0.01	1.00	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
z	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01	0.01
~~~

Prediction using a Markov chain.
Given a Markov chain model of a source, we can compute the probability that the model would produce a given string of letters by applying the chain rule. Simply stated, we walk the transitions in the Markov chain and multiply the transition probabilities. For example, the probability that “Big C, Little C” would be produced by our model, we would get the following probabilities from the transition matrix:

~~~
p(b, i) = .12
p(i, g) = .30
p(g, c) = .01
p(c, l) = .01
p(l, i) = .38
p(i, t) = .40
p(t, t) = .20
p(t, l) = .20
p(l, e) = .50
p(e, c) = .01
~~~

Multiplying them gives us 1.0588235294117648e-10. Notice that, in order to avoid getting zero-probability predictions using our simplified technique, we store .01 in our transition matrix for any bigram we don’t see in the training corpus. Also note that for larger test strings we would underflow the computer’s floating point representation and end up with zero probability. There are techniques for avoiding this problem discussed in the Additional Information listed below.

## Problem
Given sample strings for each language, generate a matrix that represents markov chain predicting which letter comes after another. Then given an input, predict what language a given input is.
**The matricies can be found under "Corpus"

## Input
A string that is one of the potential languages.

## Output
String representing which language the input most likely is.
**The string must be same name as the corresponding corpus file


**Sample Input**
> "Ou va le monde?"

**Sample output**
> "French"

**Sample Explanation**

When computed, it is most likely French.